
@testset "training.jl" begin
mktempdir() do tmp           # save all outputs to a temp dir for these tests
    @testset "test model freezing" begin
        n_layers = 4;
        n_neurons = 200
        fraction_backbone_layers = 1//2
        batch_size = 8

        # Load CLASSIFIER
        m_classifier = create_classifier_model(2, 200, 8, 20)
        model_state = JLD2.load("test_data/saved_models/classifier/saved_model.jld2", "model_state")
        Flux.loadmodel!(m_classifier, model_state)

        # load DATA
        x_train = CSV.read("test_data/sb21_22Sep25_t_x.csv", DataFrame)
        y_train = CSV.read("test_data/sb21_22Sep25_t_y.csv", DataFrame)
        x_val = CSV.read("test_data/sb21_22Sep25_t_x.csv", DataFrame)
        y_val = CSV.read("test_data/sb21_22Sep25_t_y.csv", DataFrame)

        x_train, ğ‘£_train, ğ—_ss_train, Ï_train, Îš_train, Î¼_train = preprocess_data(x_train, y_train)
        x_val, ğ‘£_val, ğ—_ss_val, Ï_val, Îš_val, Î¼_val = preprocess_data(x_val, y_val)

        # Normalise inputs
        xNorm = Norm(x_train)
        x_train = xNorm(x_train)
        x_val = xNorm(x_val)

        # Scale outputs
        ğ—Scale = MinMaxScaler(ğ—_ss_train)
        ğ—_ss_train = ğ—Scale(ğ—_ss_train)
        ğ—_ss_val = ğ—Scale(ğ—_ss_val)

        ğ‘£Scale = MinMaxScaler(ğ‘£_train)
        ğ‘£_train = ğ‘£Scale(ğ‘£_train)
        ğ‘£_val = ğ‘£Scale(ğ‘£_val)

        pp_mat = reshape(PP_COMP_adj, 6, :)
        masking_f = (clas_out, reg_out) -> (mask_ğ‘£(clas_out, reg_out[1]), mask_ğ—(clas_out, reg_out[2]))

        function loss((ğ‘£_Å·, ğ—_Å·), (ğ‘£, ğ—), x)
            return sum(abs2, ğ‘£_Å· .- ğ‘£) + sum(abs2, ğ—_Å· .- ğ—) + misfit.mass_balance_abs_misfit((descale(ğ‘£Scale, ğ‘£_Å·), descale(ğ—Scale, ğ—_Å·)), denorm(xNorm, x)[3:end,:,:], agg=sum, pure_phase_comp=pp_mat) + misfit.closure_condition((descale(ğ‘£Scale, ğ‘£_Å·), descale(ğ—Scale, ğ—_Å·)), (ğ‘£, ğ—), agg=sum)
        end
        function mae_ğ—(Å·, y)
            return misfit.mae_no_zeros(descale(ğ—Scale, Å·[2]), descale(ğ—Scale, y[2]))
        end

        loader = Flux.DataLoader((x_train, (ğ‘£_train, ğ—_ss_train)), batchsize=batch_size, shuffle=true)

        model = create_model_pretrained_classifier(fraction_backbone_layers, n_layers, n_neurons,
                                                   masking_f, m_classifier;
                                                   out_dim_ğ‘£ = 20, out_dim_ğ— = (6, 14))
        opt_state = Flux.setup(Flux.Adam(0.001), model)
        Flux.freeze!(opt_state.layers[1])  # freeze the classifier part

        model_trained, opt_state, logs, log_dir_path = train_loop(model, loader, opt_state, (x_val, (ğ‘£_val, ğ—_ss_val)), loss, 5; metrics = [mae_ğ—], save_to_subdir=tmp, show_progressbar=false)
        # test if the classifier layers have remained unchanged
        param_prior = Flux.destructure(m_classifier)
        param_post = Flux.destructure(model_trained.layers[1])
        @test param_prior == param_post
    end
end
end
