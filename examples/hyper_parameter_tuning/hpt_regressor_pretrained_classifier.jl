
using Flux
using CSV, DataFrames
using JLD2
using Sprout
using CairoMakie

n_layers = [3, 6, 9];
n_neurons = [32, 64, 128];
fraction_backbone_layers = 2//3;
batch_size = 100000;

masking_f = (clas_out, reg_out) -> (mask_ğ‘£(clas_out, reg_out[1]), mask_ğ—(clas_out, reg_out[2]));
# Load and freeze CLASSIFIER
m_classifier = create_classifier_model(3, 250, 8, 20);
model_state = JLD2.load("examples/data/saved_classifier.jld2", "model_state");
Flux.loadmodel!(m_classifier, model_state);
m_tree_classifier = Flux.setup(Flux.Adam(), m_classifier);
Flux.freeze!(m_tree_classifier);

# LOAD DATA
#-----------------------------------------------------------------------
x_train = CSV.read("data/sb21_02Oct25_train_x.csv", DataFrame);
y_train = CSV.read("data/sb21_02Oct25_train_y.csv", DataFrame);
x_val = CSV.read("data/sb21_02Oct25_val_x.csv", DataFrame);
y_val = CSV.read("data/sb21_02Oct25_val_y.csv", DataFrame);

x_train, ğ‘£_train, ğ—_ss_train, Ï_train, Îš_train, Î¼_train = preprocess_data(x_train, y_train);
x_val, ğ‘£_val, ğ—_ss_val, Ï_val, Îš_val, Î¼_val = preprocess_data(x_val, y_val);

# Normalise inputs
xNorm = Norm(x_train);
x_train = xNorm(x_train);
x_val = xNorm(x_val);

# Scale outputs
ğ—Scale = MinMaxScaler(ğ—_ss_train);
ğ—_ss_train = ğ—Scale(ğ—_ss_train);
ğ—_ss_val = ğ—Scale(ğ—_ss_val);

ğ‘£Scale = MinMaxScaler(ğ‘£_train);
ğ‘£_train = ğ‘£Scale(ğ‘£_train);
ğ‘£_val = ğ‘£Scale(ğ‘£_val);

# SETUP LOSS & METRICS
#----------------------------------------------------------------------
# Normalisation/scaling structures must live on the same device as the model is trained on
# for training on GPU move normalisers/scalers/pure_phase_comp to GPU; e.g. xNorm_gpu = xNorm |> gpu
xNorm_gpu = xNorm |> gpu;
ğ‘£Scale_gpu = ğ‘£Scale |> gpu;
ğ—Scale_gpu = ğ—Scale |> gpu;
pp_mat_gpu = reshape(PP_COMP_adj, 6, :) |> gpu;

function loss((ğ‘£_Å·, ğ—_Å·), (ğ‘£, ğ—), x)
    return sum(abs2, ğ‘£_Å· .- ğ‘£) + sum(abs2, ğ—_Å· .- ğ—) + misfit.mass_balance_abs_misfit((descale(ğ‘£Scale_gpu, ğ‘£_Å·), descale(ğ—Scale_gpu, ğ—_Å·)), denorm(xNorm_gpu, x)[3:end,:,:], agg=sum, pure_phase_comp=pp_mat_gpu)
end
# Metrics (for validation only, must follow signature (Å·, y) -> Real)
function mass_balance_metric((ğ‘£_Å·, ğ—_Å·), (_, _))
    return misfit.mass_balance_abs_misfit((descale(ğ‘£Scale, ğ‘£_Å·), descale(ğ—Scale, ğ—_Å·)), denorm(xNorm, x_val)[3:end,:,:], agg=mean)
end
function mae_ğ‘£(Å·, y)
    return misfit.mae_no_zeros(descale(ğ‘£Scale, Å·[1]), descale(ğ‘£Scale, y[1]))
end
function mae_ğ—(Å·, y)
    return misfit.mae_no_zeros(descale(ğ—Scale, Å·[2]), descale(ğ—Scale, y[2]))
end

metrics = [mass_balance_metric, mae_ğ‘£, mae_ğ—];

# TUNE IT
#-----------------------------------------------------------------------
hpt_regressor_pretrained_classifier(n_layers, n_neurons, fraction_backbone_layers, batch_size, loss,
               (x_train, (ğ‘£_train, ğ—_ss_train)), (x_val, (ğ‘£_val, ğ—_ss_val)),
               m_classifier, masking_f,
               1000, metrics,
               lr_schedule=false)

# Alternative: shared backbone model
# hpt_regressor_common_backbone(n_layers, n_neurons, fraction_backbone_layers, batch_size, loss,
#                               (x_train, (ğ‘£_train, ğ—_ss_train)), (x_val, (ğ‘£_val, ğ—_ss_val)),
#                               masking_f,
#                               10, metrics,
#                               lr_schedule=false)

# VISUALISE RESULTS
#-----------------------------------------------------------------------
log_matrix = load_hyperparam_tuning_results("hyperparam_tuning2025Dec19_1611", n_layers, n_neurons);

min_val_loss = minimum.(getfield.(log_matrix, :mean_loss));
min_mae_ğ‘£ = minimum.(getfield.(log_matrix, :mae_ğ‘£));
min_mae_ğ— = minimum.(getfield.(log_matrix, :mae_ğ—));

fig = Figure(size = (1200, 400));
ax = Axis(fig[1, 1], aspect=1.0, xlabel="n.o. hidden layers", ylabel="n.o. neurons in hidden layers");
ax.xticks = (n_layers, string.(n_layers));
ax.yticks = (n_neurons, string.(n_neurons));

hm = heatmap!(n_layers, n_neurons, min_val_loss);
Colorbar(fig[1, 2], hm; label = "min. validation loss");

ax = Axis(fig[1, 3], aspect=1.0, xlabel="n.o. hidden layers", ylabel="n.o. neurons in hidden layers");
ax.xticks = (n_layers, string.(n_layers));
ax.yticks = (n_neurons, string.(n_neurons));

hm = heatmap!(n_layers, n_neurons, min_mae_ğ‘£);
Colorbar(fig[1, 4], hm; label = "min. mae ğ‘£");

ax = Axis(fig[1, 5], aspect=1.0, xlabel="n.o. hidden layers", ylabel="n.o. neurons in hidden layers");
ax.xticks = (n_layers, string.(n_layers));
ax.yticks = (n_neurons, string.(n_neurons));

hm = heatmap!(n_layers, n_neurons, min_mae_ğ—);
Colorbar(fig[1, 6], hm; label = "min. mae ğ—");

fig