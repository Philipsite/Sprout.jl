using CSV, DataFrames
using JLD2
using Flux
using sb21_surrogate
using CairoMakie

# LOAD DATA
#-----------------------------------------------------------------------
x_train = CSV.read("examples/data/sb21_02Oct25_train_x.csv", DataFrame);
y_train = CSV.read("examples/data/sb21_02Oct25_train_y.csv", DataFrame);
x_val = CSV.read("examples/data/sb21_02Oct25_val_x.csv", DataFrame);
y_val = CSV.read("examples/data/sb21_02Oct25_val_y.csv", DataFrame);

x_train, ğ‘£_train, ğ—_ss_train, Ï_train, Îš_train, Î¼_train = preprocess_data(x_train, y_train);
x_val, ğ‘£_val, ğ—_ss_val, Ï_val, Îš_val, Î¼_val = preprocess_data(x_val, y_val);

# Normalise inputs
xNorm = Norm(x_train);
x_train = xNorm(x_train);
x_val = xNorm(x_val);

# Scale outputs
ğ—Scale = MinMaxScaler(ğ—_ss_train);
ğ—_ss_train = ğ—Scale(ğ—_ss_train);
ğ—_ss_val = ğ—Scale(ğ—_ss_val);

ğ‘£Scale = MinMaxScaler(ğ‘£_train);
ğ‘£_train = ğ‘£Scale(ğ‘£_train);
ğ‘£_val = ğ‘£Scale(ğ‘£_val);

# Setup DataLoader
loader = Flux.DataLoader((x_train, (ğ‘£_train, ğ—_ss_train)), batchsize=100000, shuffle=true);

# SETUP MODEL
#----------------------------------------------------------------------
fraction_backbone_layers = 2//3;
n_layers = 3;
n_neurons = 256;

masking_f = (clas_out, reg_out) -> (mask_ğ‘£(clas_out, reg_out[1]), mask_ğ—(clas_out, reg_out[2]));

m = create_model_shared_backbone(fraction_backbone_layers, n_layers, n_neurons, masking_f) |> gpu;

opt_state = Flux.setup(Flux.Adam(0.001), m);

# SETUP LOSS & METRICS
#----------------------------------------------------------------------
# Normalisation/scaling structures must live on the same device as the model is trained on
# for training on GPU move normalisers/scalers/pure_phase_comp to GPU; e.g. xNorm_gpu = xNorm |> gpu
xNorm_gpu = xNorm |> gpu;
ğ‘£Scale_gpu = ğ‘£Scale |> gpu;
ğ—Scale_gpu = ğ—Scale |> gpu;
pp_mat_gpu = reshape(PP_COMP_adj, 6, :) |> gpu;

function loss((ğ‘£_Å·, ğ—_Å·), (ğ‘£, ğ—), x)
    return sum(abs2, ğ‘£_Å· .- ğ‘£) + sum(abs2, ğ—_Å· .- ğ—) + misfit.mass_balance_abs_misfit((descale(ğ‘£Scale_gpu, ğ‘£_Å·), descale(ğ—Scale_gpu, ğ—_Å·)), denorm(xNorm_gpu, x)[3:end,:,:], agg=sum, pure_phase_comp=pp_mat_gpu)
end
# Metrics (for validation only, must follow signature (Å·, y) -> Real)
function mass_balance_metric((ğ‘£_Å·, ğ—_Å·), (_, _))
    return misfit.mass_balance_abs_misfit((descale(ğ‘£Scale, ğ‘£_Å·), descale(ğ—Scale, ğ—_Å·)), denorm(xNorm, x_val)[3:end,:,:], agg=mean)
end
function mae_ğ‘£(Å·, y)
    return misfit.mae_no_zeros(descale(ğ‘£Scale, Å·[1]), descale(ğ‘£Scale, y[1]))
end
function mae_ğ—(Å·, y)
    return misfit.mae_no_zeros(descale(ğ—Scale, Å·[2]), descale(ğ—Scale, y[2]))
end

# TRAIN MODEL
#-----------------------------------------------------------------------
model, opt_state, logs_t, dir = train_loop(
    m,
    loader,
    opt_state,
    (x_val, (ğ‘£_val, ğ—_ss_val)),
    loss,
    500,
    metrics = [mae_ğ‘£, mae_ğ—, mass_balance_metric],
    gpu_device = gpu_device(),
    save_to_subdir = joinpath("examples", "reg_model_simoultaneous")
);

# POST-TRAINING PLOTS
fig = post_training_plots(logs_t, dir)
