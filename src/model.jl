

const FC_SS = reshape(SS_COMP_adj, 6, Int(length(SS_COMP_adj) / 6))
const FC_SS_MASK = Sprout.SS_COMP_VARIABLE

"""
Reshape layer for regression of solid solution composition:
Reshapes vector-output of a fully-connected layer into the form Matrix(N_COMPONENTS, N_PHASES).
"""
struct ReshapeLayer
    n :: Int
    m :: Int
end
Flux.@layer ReshapeLayer
Flux.trainable(rl::ReshapeLayer) = (;)
(rl::ReshapeLayer)(x::Union{AbstractArray{Float32,3}, CuArray{Float32, 3}}) = reshape(x, rl.n, rl.m, :)


"""
Masking layer that injects fixed components into the predicted solid solution compositions.
E.g., Si in Olivine is always 1/3 molmolâ»Â¹.

This layer uses the global constants FC_SS_MASK (boolean mask of fixed components in solid solutions) and FC_SS (fixed components values).
"""
struct InjectLayer
    var_mask :: AbstractArray
    fc_vals  :: AbstractArray
end
function InjectLayer()
    return InjectLayer(FC_SS_MASK, FC_SS)
end
Flux.@layer InjectLayer
Flux.trainable(il::InjectLayer) = (;)
(il::InjectLayer)(x::Union{AbstractArray{Float32,3}, CuArray{Float32, 3}}) = x .* il.var_mask .+ il.fc_vals


"""
Mask solid solution predictions with classifier output (phase stability).
Use this function as connection in a Flux.Parallel layer:
```
Parallel(mask_ğ—,
         m_classfier,
         Chain(...)
        )
```
"""
function mask_ğ—(classifier_out, regressor_out)
    ss_stable_view = @view classifier_out[7:20, :, :]           #//NOTE - Hard-coded indices a bit hacky; classifier_out[N_PP+1, N_TOTAL,:,:] > would need to be global constants
    ss_stable_view = reshape(ss_stable_view, 1, 14, :)          #//NOTE - Same as line above
    return regressor_out .* ss_stable_view
end


"""
Mask phase fraction predictions with classifier output (phase stability).
Use this function as connection in a Flux.Parallel layer:
```
Parallel(mask_ğ‘£,
         m_classfier,
         Chain(...)
        )
```
"""
function mask_ğ‘£(classifier_out, regressor_out)
    return regressor_out .* classifier_out
end


"""
Create a flux model with a given number of (hidden) layers, and number of neurons in these hidden layers.

model = Chain(
    Dense(INPUT_DIM => N_NEURONS, relu),
    ...
    N_LAYERS
    ...
    Dense(N_NEURONS => OUTPUT_DIM, sigmoid)
)
"""
function create_classifier_model(n_layers::Integer, n_neurons::Integer, input_dim::Integer, output_dim::Integer)
    layers = []

    # First layer (input to first hidden)
    push!(layers, Dense(input_dim => n_neurons, relu))

    # Hidden layers
    for i in 2:n_layers
        push!(layers, Dense(n_neurons => n_neurons, relu))
    end

    # Output layer
    push!(layers, Dense(n_neurons => output_dim, sigmoid))

    return Chain(layers...)
end


"""
Create a flux model with the general structure:

```
model = Parallel(MASKING_FUNCTION,
                 CLASSIFIER_MODEL,
                 Chain(Dense(INPUT_DIM => N_NEURONS, relu),
                       ...
                       FRACTION_BACKBONE * N_LAYERS
                       ...
                       Dense(N_NEURONS => N_NEURONS, relu),
                       Parallel((ğ‘£, ğ—) -> (ğ‘£, ğ—),
                                Chain(Dense(N_NEURONS => N_NEURONS, relu),
                                            ...
                                            (1-FRACTION_BACKBONE) * N_LAYERS
                                            ...
                                            Dense(N_NEURONS => OUTPUT_DIM_ğ‘£)),
                                Chain(Dense(N_NEURONS => N_NEURONS, relu),
                                            ...
                                            (1-FRACTION_BACKBONE) * N_LAYERS
                                            ...
                                            Dense(N_NEURONS => *(OUTPUT_DIM_ğ—...)),
                                            ReshapeLayer(OUTPUT_DIM_REG...),
                                            InjectLayer())
                       )
                )
```

with a given number of (hidden) layers, and number of neurons in these hidden layers.
"""
function create_model_pretrained_classifier(fraction_backbone_layers::Rational{Int}, n_layers::Integer, n_neurons::Integer,
                                            masking_f::Function, m_classifier::Chain;
                                            out_dim_ğ‘£::Integer = 20, out_dim_ğ—::Tuple = (6, 14))
    # check if fraction_backbone_layers is valid
    # isinteger(n_layers * fraction_backbone_layers) || error("n_layers * fraction_backbone_layers must be an integer.")
    input_dim = size(m_classifier[1].weight, 2)
    output_dim_class = size(m_classifier[end].weight, 1)
    output_dim_class == out_dim_ğ‘£ || error("Classifier output dimension does not match out_dim_ğ‘£.")
    output_dim_regğ— = *(out_dim_ğ—...)

    # set-up regressor model
    backbone_layers = []
    n_head = round(Int, n_layers * (1-fraction_backbone_layers))
    n_backbone = n_layers - n_head
    # check if n_backbone + n_head == n_layers
    n_backbone + n_head == n_layers || error("n_backbone + n_head must equal n_layers.")
    
    for i in 1:n_backbone
        if i == 1
            push!(backbone_layers, Dense(input_dim => n_neurons, relu))
        else
            push!(backbone_layers, Dense(n_neurons => n_neurons, relu))
        end
    end
    layers_reg_ğ‘£ = []
    layers_reg_ğ— = []
    for i in 1:n_head
        push!(layers_reg_ğ‘£, Dense(n_neurons => n_neurons, relu))
        push!(layers_reg_ğ—, Dense(n_neurons => n_neurons, relu))
    end

    push!(layers_reg_ğ‘£, Dense(n_neurons => out_dim_ğ‘£))
    push!(layers_reg_ğ—, Dense(n_neurons => output_dim_regğ—))
    push!(layers_reg_ğ—, ReshapeLayer(out_dim_ğ—...))
    push!(layers_reg_ğ—, InjectLayer())

    m_regressor = Chain(vcat(backbone_layers,
                             [Parallel((ğ‘£, ğ—) -> (ğ‘£, ğ—),
                                       Chain(layers_reg_ğ‘£...),
                                       Chain(layers_reg_ğ—...)
                                      )
                             ]...
                            )...
                       )

    # create full model
    m = Parallel(masking_f,
                 m_classifier,
                 m_regressor)
    return m
end


"""
Create a flux model with the general structure:

```
model = Chain(Dense(INPUT_DIM => N_NEURONS, relu),
              ...
              FRACTION_BACKBONE * N_LAYERS
              ...
              Dense(N_NEURONS => N_NEURONS, relu),
              Parallel(Chain(Dense(N_NEURONS => N_NEURONS, relu),
                             ...
                             (1-FRACTION_BACKBONE) * N_LAYERS
                             ...
                             Dense(N_NEURONS => OUTPUT_DIM_ğ‘£, sigmoid)),
                       Chain(Parallel(MASKING_FUNCTION,
                                      Chain(Dense(N_NEURONS => N_NEURONS, relu),
                                            ...
                                            (1-FRACTION_BACKBONE) * N_LAYERS
                                            ...
                                            Dense(N_NEURONS => OUTPUT_DIM_ğ‘£)),
                                      Chain(Dense(N_NEURONS => N_NEURONS, relu),
                                            ...
                                            (1-FRACTION_BACKBONE) * N_LAYERS
                                            ...
                                            Dense(N_NEURONS => *(OUTPUT_DIM_ğ—...)),
                                            ReshapeLayer(OUTPUT_DIM_REG...),
                                            InjectLayer())
                                       )
                            )
                       )
              )
```

with a given number of (hidden) layers, and number of neurons in these hidden layers.
"""
function create_model_shared_backbone(fraction_backbone_layers::Rational{Int}, n_layers::Integer, n_neurons::Integer,
                                      masking_f::Function;
                                      input_dim::Integer = 8, out_dim_ğ‘£::Integer = 20, out_dim_ğ—::Tuple = (6, 14))
    # check if fraction_backbone_layers is valid
    # isinteger(n_layers * fraction_backbone_layers) || error("n_layers * fraction_backbone_layers must be an integer.")
    output_dim_regğ— = *(out_dim_ğ—...)

    # set-up backbone
    backbone_layers = []
    n_head = round(Int, n_layers * (1-fraction_backbone_layers))
    n_backbone = n_layers - n_head
    # check if n_backbone + n_head == n_layers
    n_backbone + n_head == n_layers || error("n_backbone + n_head must equal n_layers.")
    
    for i in 1:n_backbone
        if i == 1
            push!(backbone_layers, Dense(input_dim => n_neurons, relu))
        else
            push!(backbone_layers, Dense(n_neurons => n_neurons, relu))
        end
    end

    # set-up classifier head
    layers_class = []
    for i in 1:n_head
        push!(layers_class, Dense(n_neurons => n_neurons, relu))
    end
    push!(layers_class, Dense(n_neurons => out_dim_ğ‘£, sigmoid))

    # set-up ğ‘£ regressor head
    layers_reg_ğ‘£ = []
    for i in 1:n_head
        push!(layers_reg_ğ‘£, Dense(n_neurons => n_neurons, relu))
    end
    push!(layers_reg_ğ‘£, Dense(n_neurons => out_dim_ğ‘£))

    # set-up ğ— regressor head
    layers_reg_ğ— = []
    for i in 1:n_head
        push!(layers_reg_ğ—, Dense(n_neurons => n_neurons, relu))
    end
    push!(layers_reg_ğ—, Dense(n_neurons => output_dim_regğ—))
    push!(layers_reg_ğ—, ReshapeLayer(out_dim_ğ—...))
    push!(layers_reg_ğ—, InjectLayer())

    # create full model
    m = Chain(backbone_layers...,
              Parallel(masking_f,
                       Chain(layers_class...),
                       Chain(Parallel((ğ‘£, ğ—) -> (ğ‘£, ğ—),
                                      Chain(layers_reg_ğ‘£...),
                                      Chain(layers_reg_ğ—...)
                                     )
                             )
                      )
             )
    return m
end
